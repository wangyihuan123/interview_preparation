
04/03/2020
Hi Xinsong, This is Ben. I have a question about image preprocessing: if we use deep
learning model for image processing, like object detection or instance segmentation, do
we still need to do pre-processing like denoising and normalization? Can the model be
trained to learn those images with noise or distortion?(suppose we will have quite similar
images for training and testing). My understanding is that, the traditional denoising
methods also use different "filters" or "kernel" to smooth or remove the image. Does any
popular deep neural network consider this? 

05/03/2020
`For example, when we detect flower, we may need try to do the preprocessing to
understand the edge, the veins, etc. Depends on the use cases.` -- However, in my
opinion, the filters/kernels in the first several layers of a neural network usually are in
charge of extracting the designed feature(edge, veins). The kernels followed may help to
extract designed shapes or size and so on. So what I mean is that if understanding
edge/veins are the example for preprocessing, the kernels in the neural work do the
same things. And if sobel filter is a typical 3x3 filter in a convolutional way, why it is not
common used in the neural network? Because we can run deep learning model on GPU
which should be the faster way for convolutional operator. What do you think? 

Thank you Yihuan. I agree with you. In theory, if the neural network is complex enough,
and the data is big enough, we are able to represent anything. But in reality our data size
is relatively small, thus it may not good enough to 100% rely on the NN itself, the
preprocessing work may helps it learn faster with limited data. I think the state-of-art
models (trained from ImageNet, etc) not doing too much specific prepocessing, because
they have enough data, then they can train a good model in general.
 We can also customize the model s by adding 3x3 conv layer in the network if we define
our own structure, but some times, it makes a little bit more work if we want do transfer
learning. You know transfer learning means, we want copy the learned parameters from
a state -of-art model, which requires the structure (all, or partially) the same, at least for
the parts we want to transfer. But that's a great idea we can try. 

Hi Yihuan, that's a great question. Even though we may see a lot of Deep Learning models
did not do much preprocessing and got a good result.
 Actually, the preprocessing still needed, especially for the complex situations. For
example, when we detect flower, we may need try to do the preprocessing to understand
the edge, the veins, etc. Depends on the use cases. It would significantly improve the
result, especially with limited training data.
 For the self driving car training, the Sobel filter usually works good, it would help a lot for
the performance, the lane is quite special, it would be distinguished and remove a lot of
noise information with the preprocessing, like the filters, kernels, etc. 

BTW, some times, with the preprocessed image, the transfer learning may not always
work well, due to the color space changed, etc. 



08/03/2020
Many thanks for your reply. I will learn transfer learning later. Now just have another
question about camera calibration. In the lession 6: camera calibration. The instructor
mentioned she took 20 photos for calibration, but she didn't mention how to fuse these
20 photos with different distance and angles. All these 

My understanding for the difference between matrix/tvec/rvec from the calibrateCamera
and matrix from getPerspectiveTransform is that: 1. inputs are different: object points in
the dstcalibrateCamera are usually always the same due to the standard chessboard, but
for getPerspectiveTransform we need to provide by myself. And the inputs are four
points. 2 The result of calibrateCamera sperates the intrinisc, rvec and tvec while
getPerspectiveTransform mix them together. Am I right?

After reading description of opencv api(https://docs.opencv.org/2.
4/modules/imgproc/doc/geometric_transformations.html#undistort), I think the distcoff
and intriniscs should be the same through the 20 images, right?

Another question is about undistorting images. After camera calibration, how to apply
the distortion coefficient for every image from the camera? For instance, if the images
are captured by the camera in loop every 100ms, do I need to undistort the image every
time to get the correct image before I start to processs the image?

Are these calibration matrix generated by the mean of 20 results of cv2.cameraCalibrate()
?The cameraMatrix represents the intrinics of camera which should be the same though
all 20 chessboard calibration photos. But distort_coefficient, rotation vector and translate
vector represent extratrinsics which are all different in every single chessboard
calibration photo. Am I right?


14/03/2020
Hi Yihuan, for cameraCalibrate function, it need the array of objectPoints, imagePoints.
the imagePoints are actually N images, not 1 image. The objectPoints means N duplicated
chessboard (the same one). Wen we call the cv.calibrateCamera(objpoints, imgpoints ... it
actually inputs multiple images, and return only 1 set of parameters

So no need to run it per image

22/03/2020

Hi Yihuan, for the question "do I need to undistort the image every time to get the
correct image before I start to processs the image". The answer is YES. The undistort
shoud be part of the pipeline for each image in the video. I understand as you mention
"every 100ms", in real case, it would be running on GPU, if you run it locally and see it
works slowly, it should be fine for this project. 


Regarding the questions "calibrateCamera vs getPerspectiveTransform":
 For question (1) Yes. Right. In calibrateCamera, the chessboard is standard, only need to
provide how many rows/columns, plus multiple picture toke from the
 same camera. The parameters are calculated by algorithm, no further manual work
required.
 In getPerspectiveTransform, you need provide it yourself manually lable the points on
both picture to calculate the parameter Matrix.


For question (2)
 No. Actually the calibrateCamera does more work than getPerspectiveTransform.
 If you look at the function definition of "calibrateCamera", you may notice that beside
rvec, tvec, there is another return value "mtx", means "matrix to transform 3D to 2D",
the information may similar as the M returned from getPerspectiveTransform. Though
they may not exactly same.
 "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],
None, None)"


This is the great paper, actually the original of the theories in calibrateCamera(), it covers
a lot of topics about the meanings and calculations about the parameters. Good readings
for your reference. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.534
&rep=rep1&type=pdf
